\documentclass{aci}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{txfonts}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{colorlinks=true}
\usepackage{subcaption}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{makecell}
\usetikzlibrary{automata,positioning,arrows}
\usepackage{caption}
\usepackage{changepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\ep}{\varepsilon}
\newcommand{\eps}[1]{{#1}_{\varepsilon}}

\def\typeofarticle{Research Article} 
\def\currentvolume{3} 
\def\currentissue{1}
\def\currentyear{2022} 
\def\currentmonth{} 
\def\ppages{xx--xx} 
\def\DOI{xxxx} 
\def\Received{xx xx 2022} 
\def\Revised{xx xx 2022}
\def\Accepted{xx xx 2022} 
\def\Published{xx 2022}

\numberwithin{equation}{section}
\DeclareMathOperator*{\essinf}{ess\,inf}

\usepackage{array}
\newcolumntype{L}{>{\arraybackslash}m{8cm}}

\hyphenpenalty=10000
\usepackage{cite}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\title{Ant cuticle image classification using texture analysis: a comparative study}

\author{%
    Noah Gardner\affil{1},
    John Paul Hellenbrand\affil{2},
    Anthony Phan\affil{1},
    Haige Zhu\affil{1},
    Zhiling Long\affil{4},
    Min Wang\affil{3},
    Clint Penick\affil{2},
    and Chih-Cheng Hung\affil{1}\corrauth
}%

% \shortauthors is used in copyright information in the end of the paper
\shortauthors{the Author(s)}

\address{%
    \addr{\affilnum{1}}{Laboratory of Machine Vision and Security Research, %
        College of Computing and Software Engineering, Kennesaw State %
        University, Marietta GA, USA}%
    \addr{\affilnum{2}}{College of Science and Mathematics, Kennesaw State
        University, Kennesaw, GA, USA}%
    \addr{\affilnum{3}}{Department of Mathematics, Kennesaw State University,
        Kennesaw, GA, USA}%
    \addr{\affilnum{4}}{Department of Mathematics, Science, and Informatics,
        Mercer University, Atlanta, GA, USA}%
}
\corraddr{chung1@kennesaw.edu}

\editor{Pasi Fr\"{a}nti}

\begin{abstract}
    There is a large variety of ant species, and most species are diverse in
    terms of size, shape, behaviors, and especially cuticle textures. However,
    the significance of ant cuticle texture is not widely researched. Ant
    cuticle texture presumably provides some type of function, and therefore is
    useful to research for ecological applications and bioinspired designs. This
    research employs image texture analysis and deep machine learning to
    automatically group similar ant species based on morphological traits. We
    provide a comparative study of the performance of general image
    classification and texture analysis methods on ant head images. We evaluate
    the results of the classification methods with modern visualization
    techniques. Our results show that deep learning methods are applicable for
    classifying ant cuticle textures. We also show that the performance of the
    deep texture analysis methods performs better than the deep classification
    methods.
\end{abstract}
\keywords{texture analysis; image processing; classification; machine learning;
    ant cuticle images; ecology}
\maketitle

\section{Introduction}

Insects compose half of biodiversity and rank among the most dominant organisms
in terrestrial ecosystems \cite{sheikh_diverse_2017}. A key factor for the
ecological success of insects is their exoskeleton, also known as cuticle. The
cuticle protects insects from predation, provides structural support, prevents
desiccation, and serves as a canvas for advertising visual and chemical signals
\cite{gullan_insects_2009}. Research has heavily focused on the macrostructures
and internal chemical components that make the exoskeleton functional and more
recent work is being done to understand the functional aspects of external
cuticle micro sculpturing \cite{muthukrishnan_insect_2020,
    gunderson_insect_1989, watson_diversity_2017}.

Due to the extensive number of insect species, manual exploration of
insect-based information is difficult and often requires specialized expertise.
Therefore, automated entomology is gaining attraction by both biologists and
computer scientists and is expected to be a major contribution to the future of
insect-based research \cite{martineau_survey_2017}. One of the most commonly
used data types for insect analysis is image data. To develop an image-based
system for insect analysis, we can take advantage of existing work in general
image analysis methods.

We examine ants (\textit{Formicidae}) as they display an extreme diversity of
cuticle micro sculpturing across all subfamilies. Sculpturing ranges from
parallel longitudinal ridges to deep oval impressions to erratic protuberances.
The sculpturing has arisen convergently and independently throughout ant's
evolutionary history, which suggests some inherent function. Cuticle sculpturing
on ants may help increase strength and rigidness, resist abrasion, increase
internal and external surface area, resist microbial growth, and rear beneficial
anti-biotic producing bacteria \cite{johnson_effect_2011,
    bruckner_relationship_2017, currie_coevolved_2006}. These specific functions may
be associated with certain sculpturing types. In order to analyze those
functions, it is necessary to segment the image and group similar textures for
further analysis.

Our primary contribution is the custom dataset used to explore the relationship
between ant cuticle texture and automated texture analysis methods. We compare
classical and modern image classification methods and texture analysis methods
by evaluating their performance on our dataset.

\section{Related work and literature review}

In this section we will review the literature for ant sculpturing identification
and texture analysis. Additionally, we discuss the work related to automated
insect identification.

\subsection{Sculpturing identification}
Taxonomists have developed extensive terminology describing ant cuticle
sculpturing as it is often a useful diagnostic trait to distinguish between
closely related ant species \cite{blaimer_taxonomy_2019,fisher_ants_2007}. The
definitive text on ant cuticle terminology — \textit{The Glossary of Surface
    Sculpturing} — contains over 100 terms to describe the cuticle sculpturing
patterns of ants \cite{harris_glossary_1979}.

Cuticle sculpturing in ants has been explored thoroughly from a taxonomic
perspective; however, the function of nano and microstructures on insect
exoskeletons is a developing topic in entomology. Watson et al. review the
literature of cuticle nano and microstructure function and propose 21 possible
functions associated with these structures \cite{watson_diversity_2017}. Many of
these functions are related to structures not found in ants such as scales and
nanostructures. The review does include functions that may relate to ants such
as friction control, enhanced surface area, and increased hardness. Watson et
al. also describe seven types of cuticle structures ranging from hairs and
scales to nano and micro structures \cite{watson_diversity_2017}. The cuticle
sculpturing of ants seems to fall within one type - complex microstructures.

The five broad functional groupings developed that describe the complex
microstructures found on ants were derived from reviewing the variation of
cuticle sculpturing across ants. These functional groupings were also reflected
in Harris as many terms could be grouped together based on similar definitions
and comparing the scanning electron microscope photographs provided in the
publication \cite{harris_glossary_1979}.

There is often substantial overlap among terms, and closely related cuticle
patterns are likely to be functionally similar. For example, the definition for
“imbricate” is, “partly overlapping and appearing like shingles on a roof or
scales on a fish,” which is difficult to distinguish from the definition of
tesselate, “made up of squares like a chess board, either in sculpturing or in
color.” However, we note that the terms used to describe the cuticle patterns
are similar to the terms used to describe textural patterns in texture analysis.
In the next subsection, we review some literature of image texture analysis.

\subsection{Texture analysis}

Image texture analysis is an important part of image interpretation as many
objects appearing in the textural patterns either partially or completely inside
an image. This phenomenon appears frequently in many images we encountered in
our environment as almost everybody is using the iPhone to capture the photos.
Scientists are also studying their subjects depending on taking the images
remotely such as remotely sensed satellite images. With an abundant number of
textural images surrounding us, it is essential to have a reliable automatic
tool for the textural image interpretation.

Classical computer methods are developed for textural images based on the
spatial relationship of gray levels of pixels encoded in the texture
\cite{haralick_image_1985,hung_image_2019}. Those methods by de-facto follow the
steps used in the statistical pattern recognition
\cite{fukunaga_introduction_2013}. Therefore, textural features are extracted,
and the classification and segmentation methods are used for the interpretation
either using the supervised or unsupervised approaches. The interpretation
method is usually taken and modified from the traditional pattern recognition
methods. For example, the gray-level co-occurrence matrix (GLCM) and local
binary patterns (LBP) are two of methods for extracting features
\cite{haralick_textural_1973, goos_gray_2000}.

Similar to the traditional pattern recognition, classical image texture analysis
methods can be grouped into four categories: statistical, structural,
model-based, and transform-based methods \cite{bharati_image_2004}. Among all of
the categories, the statistical method is often used by employing the method
from the pattern recognition principle.  In doing so, an image texture is
represented as feature vectors and then fed into the algorithm. Markov random
field models are also studied for textural image interpretation
\cite{hassner_use_1981, cross_markov_1983}. The transform-based methods use some functions to
decompose an image texture into a set of basic feature images. Gabor filters and
Wavelet expansions are two of the widely used approaches
\cite{bovik_multichannel_1990}.

While most feature vectors are established based on a single pixel, Liu et al.
gave a comprehensive survey on textural characterization in which they call this
feature extraction as texture representation \cite{liu_bow_2019}. Their survey
on Bag of Words and Convolutional Neural Networks (CNNs)
\cite{krizhevsky_imagenet_2017} are the spatial relationship concepts. The
K-views was developed for taking the spatial information in the clustering
approach \cite{hung_image_2019}.

In the past decade, deep learning using CNNs has emerged as the mainstream
technology for image analysis. Following this trend, various CNN-based network
architectures have been designed to specifically characterize textural images.
Among these, the Fisher-vector CNN descriptor (FV-CNN) proposed by Cimpoi et
al. \cite{cimpoi_deep_2015} is widely accepted as one of the most important
pioneering works. It applies FV pooling to deep features obtained via a CNN
pre-trained using the ImageNet \cite{krizhevsky_imagenet_2017} to obtain encoded features
for texture classification. Although it is capable of achieving much improved
classification accuracy comparing to traditional hand-crafted texture features,
FV-CNN does not support an end-to-end learning, where feature extraction,
encoding, and classification are separated from each other. To achieve an
end-to-end learning, Zhang et al. \cite{zhang_deep_2017} proposed the deep texture
encoding network (DeepTEN), in which a novel texture encoding layer is added to
a standard CNN architecture. Then, Xue et al. \cite{xue_deep_2018} constructed the
deep encoding pooling network (DEP), which improves over DeepTEN by integrating
local spatial characteristics into the texture representation. Based on DeepTEN
and DEP, Hu et al. \cite{hu_multi_2019} further developed the multi-level texture
encoding and representation (MuLTER) network, which embeds a learnable encoding
module at each convolutional layer so that encoding is performed for both
low-level and high-level features, yielding a multi-level texture
representation.

Other network architectures for end-to-end texture learning are also available.
For example, in the deep multiple-attribute-perceived network (MAP-Net)
\cite{zhai_deep_2019}, multiple perceptual attributes are progressively learned
in a mutually reinforced manner through multiple branches. In the deep
structure-revealed network (DSR-Net) \cite{zhai_deep_2020}, inherent structural
representation for a texture pattern is obtained by employing a primitive
capturing module to learn spatial primitives and a dependency learning module to
capture the dependency among the primitives. In \cite{mao_deep_2021}, a residual
pooling layer consisting of a residual encoding module and an aggregation module
is used to generate discriminative features of low dimensions. In
\cite{peeples_histogram_2021}, a histogram layer is designed to compute local
spatial distribution of CNN features. In \cite{chen_deep_2021}, an innovative
aggregation module is presented to exploit statistical self-similarity across
layers. All these architectures customize the standard CNN structure to
accomplish the characterization of certain spatial, visual, or statistical
nature unique to textural images.

Compared with the traditional method in which a kernel must be designed by an
engineer for extracting features, the deep learning networks can automatically
extract the features through the training. In addition, the deep learning
networks achieve the higher accuracy than that of the classical approaches,
although the deep networks require much more data for training.

\subsection{Insect classification}
In this section, we provide an overview of some insect classification methods.
Proposed insect classification methods seek to classify insects at different
hierarchical levels, such as species, genus, family, and order. Additionally,
some methods may classify insects at a combination of different hierarchical
levels. Insect classification methods can be applied to a variety of fields. In
agriculture, insect classification methods can be used to identify the presence
of pest insects in crops, which can inform crop managers in their choice of
pesticides and help prevent crop loss \cite{liu_pestnet_2019,
    kasinathan_machine_2021}.

Feng et al. \cite{feng_automated_2013} apply an automated system to classify
moth images based on semantic related visual attributes, which are defined as a
pattern on the moth wings. Feng et al. \cite{feng_automated_2013} use a custom
texture descriptor based on the combination of GLCM and \textit{scale-invariant
    feature transform} (SIFT) features \cite{gotlieb_texture_1990,
    lowe_distinctive_2004}. The method proposed by Feng et al.
\cite{feng_automated_2013} is used to classify 50 different moth species across
8 families \cite{feng_automated_2013}. The results from Feng et al.
\cite{feng_automated_2013} suggest that traditional feature extraction
techniques for the semantic visual attributes of the moth wings are sufficient
for training a classifier to classify an image between 10 randomly selected moth
species.

Urteaga et al. \cite{urteaga_scorpions_2016} use machine learning methods in
order to classify images between two different scorpion species:
\textit{Centruroides limpidus} and \textit{Centruroides noxius}. After applying
background distinction based on dynamic color threshold, Urteaga et al.
\cite{urteaga_scorpions_2016} apply feature extraction to extract features from
the separated scorpion image such as aspect ratio, rectangularity, and
compactness. Urteaga et al. \cite{urteaga_scorpions_2016} apply three different
classification models to classify the image as one of the species: Artificial
Neural Network, Regression Tree, and Random Forest classifiers
\cite{urteaga_scorpions_2016}. The results from Urteaga et al.
\cite{urteaga_scorpions_2016} show that after background removal,
characteristics from the entire body of the scorpion can be used to create a
binary classifier that can classify the image as one of the two species.

Lim et al. \cite{lim_performance_2017} apply a CNN-based algorithm for insect
classification. Lim et al. \cite{lim_performance_2017} classify a subset of
insect species and families based on the classes available in the ImageNet
dataset. ImageNet is a widely used dataset of images labeled by experts with
millions of images and thousands of categories \cite{deng_imagenet_2009}. In the
ImageNet dataset, there are some categories that specify the class of the insect
on a species level, \textit{e.g.} \textit{monarch butterfly} and \textit{ringlet
    butterfly} as well as some categories that specify the class of the insect on a
family level, \textit{e.g.} \textit{ant}, \textit{fly}, and \textit{bee}
\cite{deng_imagenet_2009}. Lim et al. \cite{lim_performance_2017} use a modified
AlexNet architecture and experiment with different numbers of kernels and their
effect the performance of the model. Glick et al. \cite{glick_insect_2016}
employ a similar approach by classifying 277 insect classes from ImageNet using
a hierarchical convolutional neural network. The results from Lim et al.
\cite{lim_performance_2017} and Glick et al. \cite{glick_insect_2016} suggest
that a CNN is capable of differentiating between different hierarchical classes
of insects.

\section{Methodology}

In this section, we provide an overview of the methodology used in this paper.
We describe the steps used to prepare the custom dataset. Finally, we describe
the algorithms applied and experimental setup to obtain the results.

\subsection{Dataset preparation} % preparation generation

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{thesis_assets/images/rough_collage.png}
    \caption{Examples of rough cuticle texture ant images in the dataset after
        center cropping, from AntWeb \cite{perrichot_antweb_2012}.}
    \label{fig:rough-cuticle-texture}
\end{figure}

In this section, we describe the creation of the custom dataset used in this
research. In our dataset, we use ant head images from AntWeb
\cite{perrichot_antweb_2012} and define two categories for them based on the
appearance of the cuticle texture: \textit{rough} and \textit{smooth}. We also
refer to these ant head images, which describe the pose of the ant, as ant
cuticule images, due to the clear cuticle texture present on the head. Some
randomly selected images from each category are shown in Figures
\ref{fig:rough-cuticle-texture} and \ref{fig:smooth-cuticle-texture}.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{thesis_assets/images/smooth_collage.png}
    \caption{Examples of smooth cuticle texture ant images in the dataset after
        center cropping, from AntWeb \cite{perrichot_antweb_2012}.}
    \label{fig:smooth-cuticle-texture}
\end{figure}

To begin, a master spreadsheet was created with the 2,499 different ant species
to be identified for the primary dataset. The team was trained to identify
cuticle sculpturing through a process which consisted of one 45-minute
introductory lesson explaining the project and texture categories. Then, the
team was given a training set of photos to identify from the genus
\textit{Polyrhachis}. The sculpture identification protocol describes the two
primary categories: \textit{rough} and \textit{smooth}.

Initially, the sculpture identification protocol had 8 subcategories of cuticle
texture, including dimpled, ridged, and differing levels of smooth texture. For
simplicity, we work only with the two main categories. The training set
identifications were reviewed together as a group by the assistants. Once
training was complete, assistants were assigned the same genera of ants to
identify independently each week. A weekly meeting was held to discuss
identifications and assign new ones. These identifications were collected in the
master spreadsheet and the identifications were assigned to individual ant
species on a majority basis.

To collect the images, the assistants followed the taxonomy information
available in the master spreadsheet to the appropriate AntWeb page
\cite{perrichot_antweb_2012}. In many cases, there are multiple ant head images
of the same species, and occasionally there are multiple image resolution
available from a single image. To simplify the data collection process, the
assistants were instructed to download the first ant head image of the species
being identified in the highest resolution possible. Each image was named with
an identifier that corresponds with the row number in the master spreadsheet.
The same ant head images that were downloaded in the data collection phase were
the same ones used in the sculpture identification protocol. Ant species which
did not have any images of the head were excluded from the dataset.
Additionally, ant species which only had a head image of a queen ant were
excluded from the dataset.

Ant specimen images taken from AntWeb \cite{perrichot_antweb_2012} are created
by different photographers and therefore have different attributes, such as
environment, resolution, and lighting. In the ant head images, the ant head is
in the center of the image and the body is pointing away from the camera. The
focus of the ant head image is centered on the head, with the background and
image artifacts from the ant body typically blurred. In most ant head images,
there is a bar which indicates the scale of the image due to the variety in the
sizes of different ant species. In a few ant head images, there exists some text
denoting the specimen identifier and other information. In terms of texture,
some ant specimens are very old, so their head images have other abnormalities
such as cracks in the cuticle and the presence of dust.

Due to the variety of the ant head image attributes, we apply simple
preprocessing before the images are used in our model. We want the images to
have a uniform size for simplicity in our classification process. Since the ant
head images are typically centered in the image, we apply a center crop to each
image to create a square image of the same size. Once the image is square, we
resize each image to a fixed size of 256x256 pixels. We leave other
discrepancies in the images untouched.

Our custom dataset of ant head images contains 2,499 images. 1072 samples of
rough textured ant cuticle textures comprise 43\% of the dataset. The remaining
1427 samples of smooth textured ant cuticle textures comprise 57\% of the
dataset. To handle the imbalance of the dataset, we apply undersampling for each
class for the training dataset. By using random stratified sampling, we
construct a training set with 800 images per class. The remaining images are
randomly split between test and validation, which turns out to roughly a
60\%/20\%/20\% train, test, and validation data split. With 272 rough samples
and 627 smooth samples left over after the stratified split, the test dataset
has roughly 136 rough samples and 313 smooth samples. Since these leftover
samples are split with code by 50\% there will be some rounding variance and
therefore the test dataset built at run-time will not always have exactly the
same number of samples.

\subsection{Classical texture analysis methods}

The K-means algorithm is one of widely used clustering algorithms in the pattern
recognition \cite{lloyd_least_1982}. It is a single pixel based classification
algorithm for images. The K-views is an algorithm that uses several
characteristic views for the classification of images \cite{hung_use_2002}. The
K-views algorithm is suitable for classifying image textures that have basic
local patterns repeated in a periodic manner. In contrast to the K-means,
K-views looks at the neighboring pixels for providing the spatial relationship
for classification.  Several variations of the basic K-views model have been
proposed \cite{yang_image_2003, lan_improved_2010}.

For the feasibility study in this ant image datasets, we use both K-means and
K-views in our experiments as both are statistical clustering methods. The
experiments may indicate how well the statistical approach will do in this type
of textural images. In pre-processing the dataset, we apply the Gabor filter to
transform the gray-scale ant image into four Gabor-filtered channels, subtract
the gray-scale image from each channel, take the absolute value of the resulting
images, and then normalize them. This pre-processing will highlight some of
characteristics features such as edge-like in the image. This step is very
useful in feeding the datasets to the K-views algorithms. A 10-channel Gabor
filter is applied in the pre-processing step for the K-means algorithm.

\subsection{Deep learning models used for the experiments}

Our first model is \textit{visual geometry group} (VGG), a convolutional neural
network that takes advantage of very small convolutional filters in a deep
network architecture \cite{simonyan_very_2015}. We compare four architectures of
VGG: VGG11, VGG13, VGG16, and VGG19. The primary difference between the
architectures is the number of layers in each model. Our second model is
\textit{residual network} (ResNet), a deep network architecture that includes
shortcut connections between layers (residual connections) \cite{he_deep_2015}.
We compare three architectures of ResNet: ResNet18, ResNet50, and ResNet101.
Again, the primary difference between the architectures is the number of layers
in each model.

For our ResNet models, we have two versions: randomized and pretrained. The
randomized version is the same architecture, but the weights are randomly
initialized. The pretrained version has weights from training on the CIFAR
dataset, an image dataset with 1000 classes. In this case, we are fine-tuning
the pretrained model. For VGG, we are only using the randomized version. The
base VGG architecture also has an output layer of size 1000. Since we are
working with a binary classification problem, we modify the architecture for all
models to have an output layer of size 2. Each model is trained over 100 epochs,
using stochastic gradient descent with momentum. The batch size is set to 16
images. We apply a learning rate of 0.001 and momentum parameter of 0.9.

% \colorbox{green}{\parbox{\dimexpr\textwidth-2\fboxsep}{TODO: I suggest that at
%         least one deep learning based texture classification algorithm be
%         applied to the dataset for the comparative study. When this is the
%         case, one more subsection should be included here to describe
%         that/those algorithm(s).}}

\subsection{Evaluation}

We evaluate the performance of the models according to standard evaluation
methods. Since we are working with a binary classification problem, we use a
standard confusion matrix to evaluate the accuracy, precision, and F1 score. We
also apply Grad-CAM with manual inspection to visualize the activation weights
for classified images to visualize which features lead to the classification
result. Finally, we apply t-SNE to visualize the separation learned for the
model to further analyze the classifications made by the model.

\section{Results}
\subsection{Environment}

Experiments are run on an Ubuntu 18.04 LTS Lambda Labs GPU server. The server
contains 8 NVIDIA GeForce RTX 2080 Ti graphics cards with 12GB of memory each.
The server uses an Intel Xeon Silver 4116 with 48 total threads and maximum
frequency of 3.000 GHz, and has 256GB of RAM.

\subsection{Evaluation results}

To begin, we share the results on the each algorithm specified in the
methodology section using the evaluation metrics described. The results are
shown in Table \ref{tab:results}. It should be noted that due to the class
imbalance in the dataset, the F1 score is the preferable metric to the accuracy.
Therefore, the results have been sorted by F1 score in descending order.

\begin{table}[h]
    \centering
    \caption{Results}
    \input{assets/results.tex}
    \label{tab:results}
\end{table}

\section{The analysis with visualization}

The results in the previous sections show that the fine-tuned ResNet models
outperform the VGG and randomly initialized ResNet models on the task of ant
head image classification. Additionally, the DRP models perform better than the
the general deep learning models VGG and ResNet on the task of ant head image
classification.The fine-tuned DRP single-layer model with auxiliary classifier
performed the best with an average F1 score of 0.92. We further analyze the
separation learned by both ResNet101 models in the following section.

% \colorbox{green}{(TODO: DRP is best performing model, it should be included in
%     visualization.)}

\subsection{t-SNE Visualization}

\begin{figure}[h]
    \centering
    \begin{subfigure}{.45\textwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/plots/resnet101_gt_tsne.png}
        % \caption{Ground truth labels}
    \end{subfigure}
    \begin{subfigure}{.45\textwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/plots/resnet101_pred_tsne.png}
        % \caption{Predicted labels}
    \end{subfigure}
    \caption{t-SNE visualization of the embeddings of the second to last layer
        of the randomly initialized ResNet101 model trained on ant head image
        dataset.}
    \label{fig:resnet101_tsne}
\end{figure}

In this section, we provide visualization of the fine-tuned ResNet101 model and
the randomly initialized ResNet101 model using t-SNE dimensionality reduction.
First, we run the dataset preprocessing method and initialize both models. Then,
both models are trained according to the training parameters and the state of
each model is saved. To visualize the deep extracted features, we modify each
model to obtain the embeddings of the second to last layer. Then, we use the
t-SNE algorithm to reduce the dimensionality of the embeddings to 2 dimensions.
We plot side-by-side the ground truth and predicted labels for each model.
Figure \ref{fig:resnet101_tsne} shows the results of the trained randomly
initialized model and Figure \ref{fig:fresnet101_tsne} shows the results of the
fine-tuned model.

\begin{figure}[h]
    \centering
    \begin{subfigure}{.45\textwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/plots/fresnet101_gt_tsne.png}
        % \caption{Ground truth labels}
    \end{subfigure}
    \begin{subfigure}{.45\textwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/plots/fresnet101_pred_tsne.png}
        % \caption{Predicted labels}
    \end{subfigure}
    \caption{t-SNE visualization of the embeddings of the second to last layer
        of the fine-tuned ResNet101 model trained on ant head image dataset.}
    \label{fig:fresnet101_tsne}
\end{figure}

Based on the visual results of the t-SNE visualization, we can see that the
fine-tuned model learned a stronger separation of the two classes, which
reinforces the results that the fine-tuned model received a higher average
accuracy.

\subsection{GradCAM visualization}

In this section, we provide some visual analysis of some correctly and
incorrectly classified images using GradCAM. We provide two categories and two
subcategories in our analysis. The two categories are correct and incorrect
classification. Regardless of the feature activation map, the correctly
classified images have the same predicted label as the ground truth, and
incorrectly classified images have different predicted labels. The two
subcategories are expected and non-expected feature activation. In the expected
case, the features that are used to compute the classification are the same as
the features used by the assistants in the sculpture identification process. In
general, the features used by the assistants are the textures of the cuticle on
the ant head. In the non-expected case, the features used to compute the
classification are not from the head, for example, from the background,
extraneous text, or the body of the ant. We used randomly selected images from
the dataset and the fine-tuned ResNet101 model to perform the analysis. We show
the GradCAM results in Figures \ref{fig:correct_ideal},
\ref{fig:correct_nonideal}, \ref{fig:incorrect_ideal}, and
\ref{fig:incorrect_nonideal}. The left image shows the preprocessed image input
to the model. The right image shows the GradCAM output based on the
classification. Four specimens were selected randomly from each category and
subcategory.

Correctly classified images which use the expected features show the ideal
performance of the model. Incorrectly classified images which use the expected
features should be further analyzed. In essence, the model in this situation
knows \textit{where} to look, but not \textit{what} to look for. In Figure
\ref{fig:incorrect_ideal_84}, the features activated are mostly in the correct
location on the ant head, and the rough texture is clearly visible, yet the
model predicts the incorrect class \textit{smooth}. Similarly in Figure
\ref{fig:incorrect_ideal_177}, the features activated are also mostly in the
correct location, yet the model predicts the incorrect class \textit{smooth}. In
this case, it may be due to the pose of the ant being slightly different from
the average pose. In the incorrectly classified images with non-expected
features, analysis shows that the model is unable to find \textit{where} to
look, and obtains feature information from other parts of the ant or the
background. Cases where the image was correctly classified using the
non-expected features can basically be seen as noise. In order to further
analyze this class, we should introduce some parameter such as model confidence
to examine further.

\newcommand{\subwidth}{0.35\textwidth}
\begin{figure}
    \centering
    \begin{subfigure}{\subwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/gradcam/correct_ideal/518.png}
        \caption{Ground truth: smooth}
        \label{fig:correct_ideal_518}
    \end{subfigure}
    \begin{subfigure}{\subwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/gradcam/correct_ideal/808.png}
        \caption{Ground truth: smooth}
        \label{fig:correct_ideal_808}
    \end{subfigure}
    \begin{subfigure}{\subwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/gradcam/correct_ideal/842.png}
        \caption{Ground truth: rough}
        \label{fig:correct_ideal_842}
    \end{subfigure}
    \begin{subfigure}{\subwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/gradcam/correct_ideal/1091.png}
        \caption{Ground truth: rough}
        \label{fig:correct_ideal_1091}
    \end{subfigure}
    \caption{Correctly classified images using expected features.}
    \label{fig:correct_ideal}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{\subwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/gradcam/correct_nonideal/346.png}
        \caption{Ground truth: rough}
        \label{fig:correct_nonideal_346}
    \end{subfigure}
    \begin{subfigure}{\subwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/gradcam/correct_nonideal/1554.png}
        \caption{Ground truth: rough}
        \label{fig:correct_nonideal_1554}
    \end{subfigure}
    \begin{subfigure}{\subwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/gradcam/correct_nonideal/1694.png}
        \caption{Ground truth: rough}
        \label{fig:correct_nonideal_1694}
    \end{subfigure}
    \begin{subfigure}{\subwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/gradcam/correct_nonideal/388.png}
        \caption{Ground truth: smooth}
        \label{fig:correct_nonideal_388}
    \end{subfigure}
    \caption{Correctly classified images using non-expected features.}
    \label{fig:correct_nonideal}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{\subwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/gradcam/incorrect_ideal/84.png}
        \caption{Ground truth: rough}
        \label{fig:incorrect_ideal_84}
    \end{subfigure}
    \begin{subfigure}{\subwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/gradcam/incorrect_ideal/86.png}
        \caption{Ground truth: smooth}
        \label{fig:incorrect_ideal_86}
    \end{subfigure}
    \begin{subfigure}{\subwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/gradcam/incorrect_ideal/177.png}
        \caption{Ground truth: smooth}
        \label{fig:incorrect_ideal_177}
    \end{subfigure}
    \begin{subfigure}{\subwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/gradcam/incorrect_ideal/197.png}
        \caption{Ground truth: smooth}
        \label{fig:incorrect_ideal_197}
    \end{subfigure}
    \caption{Incorrectly classified images using expected features.}
    \label{fig:incorrect_ideal}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{\subwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/gradcam/incorrect_nonideal/1.png}
        \caption{Ground truth: rough}
        \label{fig:incorrect_nonideal_1}
    \end{subfigure}
    \begin{subfigure}{\subwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/gradcam/incorrect_nonideal/22.png}
        \caption{Ground truth: rough}
        \label{fig:incorrect_nonideal_22}
    \end{subfigure}
    \begin{subfigure}{\subwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/gradcam/incorrect_nonideal/61.png}
        \caption{Ground truth: rough}
        \label{fig:incorrect_nonideal_61}
    \end{subfigure}
    \begin{subfigure}{\subwidth}
        \includegraphics[width=1\linewidth]{thesis_assets/gradcam/incorrect_nonideal/204.png}
        \caption{Ground truth: rough}
        \label{fig:incorrect_nonideal_204}
    \end{subfigure}
    \caption{Incorrectly classified images using non-expected features.}
    \label{fig:incorrect_nonideal}
\end{figure}

\FloatBarrier
\section{Conclusion}
Ant cuticle texture presumably has some function, but without the proper tools,
evaluating the function based on thousands of species is infeasible. We have
shown in this work that a deep learning approach and classical image texture
analysis methods can be used to automatically categorize ants based on their
cuticle texture, therefore supporting research on the evaluation of the function
in future work. Our categorization system is novel in the field of automated
insect identification due to the broad number of species captured by it.
Additionally, a model that is pre-trained on a diverse image task such as ResNet
can be transferred to our domain of texture analysis. However, a deep learning
algorithm created specifically for the domain of texture analysis had the best
results in our experiments. In future work, we will continue to explore texture
classes present in the dataset which are not captured by the binary class
system. All code is publicly available on GitHub
(https://github.com/ngngardner/cuticulus).

\bibliographystyle{AIMS}
\bibliography{main.bib,article.bib}
\end{document}
